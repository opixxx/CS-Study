# 운영체제

> 이 글은 [보초님 깃허브 레포](https://github.com/VSFe/Tech-Interview)를 참고해서 공부한 글입니다

# 1. 시스템 콜이 무엇인지 설명해 주세요.

유저 모드의 응용프로그램이 커널 모드의 OS 기능을 호출하기 위한 인터페이스이다.

![syscall](https://github.com/user-attachments/assets/fc720165-8151-4bb4-9b3c-2add57c5f508)

## 우리가 사용하는 시스템 콜의 예시, 유형을 설명해주세요
- 프로세스 제어
  - fork(), exec(), exit(), wait()
- 파일 관리
  - open(), read(), write(), close(), unlink()
- 장치 관리
  - ioctl(), read(), write()
- 정보 유지
  - getpid(), alarm(), sleep()
- 통신
  - pipe(), socket()
- 메모리 관리
  - brk(), mmap()

## 시스템 콜이, 운영체제에서 어떤 과정으로 실행되는지 설명해 주세요.
1. 응용 프로그램이 시스템 콜 호출
2. 라이브러리 함수가 시스템 콜 번호 및 인수를 레지스터에 설정
3. 트랩 명령을 통해 유저 모드에서 커널 모드로 전환
   - CPU 내부의 mode bit 을 11(user) 에서 00(kernel) 으로 변경하고, 커널의 Trap Handler를 호출
4. 커널이 시스템 콜 번호를 확인하고 해당 커널 함수 실행
   - Trap Handler(=System call handler)가 전달된 정보에 따라 커널 내부 함수를 호출
5. 커널 함수가 요청된 작업을 수행
6. 결과를 레지스터에 저장하고 유저 모드로 복귀
7. 응용 프로그램이 시스템 콜의 반환값을 받아서 이후 작업을 처리

## 운영체제의 Dual Mode 에 대해 설명해 주세요.
운영체제는 보안과 하드웨어 자원의 보호를 위해 유저 모드와 커널 모드로 나뉜다. 우리가 사용하는 애플리케이션은 유저 모드에서 실행되고, 하드웨어 자원의 사용을 위해서는 커널 모드로 가야한다.
## 왜 유저모드와 커널모드를 구분해야 하나요?
사용자와 운영체제는 시스템 자원을 공유합니다. 이때 사용자에게 제한을 두지 않으면 운영체제의 자원을 망가뜨릴 수 있다. 따라서 운영체제의 자원을 보호하기 위해 구분해서 사용자가 시스템 자원 접근을 제한한다.
## 서로 다른 시스템 콜을 어떻게 구분할 수 있을까요?
시스템 콜을 고유한 번호를 가지고 있다. 프로세스가 시스템 콜을 호출할 때 시스템 콜 번호를 특정 레지스터에 저장하여 커널이 어떤 시스템 콜을 요청했는지 확인할 수 있게 한다.

# 2. 인터럽트가 무엇인지 설명해 주세요.
프로그램을 실행 중에 예기치 않은 상황이 발생할 경우 현재 실행 중인 작업을 멈추고 발생된 상황에 대한 우선 처리가 필요함을 CPU에게 알리는 것
## 인터럽트는 어떻게 처리하나요?

![interrupt-suhyun](https://github.com/user-attachments/assets/83c9771f-865e-4bcf-9062-d02a2c85e9ec)

1. CPU 인터럽트 요청 신호를 보낸다.
2. CPU는 실행 사이클이 끝나고 명령어를 인출하기 전 항상 인터럽트 여부를 확인한다.
3. CPU는 인터럽트 요청을 확인하고 인터럽트 플래그를 통해 현재 인터럽트를 받아들일 수 있는지 여부를 확인한다.
4. 인터럽트를 받아들일 수 있다면 CPU는 지금까지의 작업을 백업한다.(스택 영역)
5. CPU는 인터럽트 벡터를 참조하여 인터럽트 서비스 루틴을 실행한다.
6. 인터럽트 서비스 루틴 실행이 끝나면 4에서 백업해 둔 작업을 복구하여 실행을 재개한다.

## Polling 방식에 대해 설명해 주세요.
다른 프로세스에게 CPU를 양도하지 않고, 하드웨어 장치가 동작을 완료할 때까지 계속 루프를 돌면서 하드웨어 상태를 체크하는 방식이다.
단순한 I/O 장치에서 사용한다. CPU가 계속 장치의 상태를 확인해도 크게 부담이 되지 않기 때문이다.
## HW / SW 인터럽트에 대해 설명해 주세요.

| **구분**    | HW 인터럽트                    | SW 인터럽트               |
|:---------:|:--------------------------:|:---------------------:|
| **발생 원인** | 외부 하드웨어 장치에 의해 발생          | 프로그램 내에서 발생           |
| **발생 시점** | 비동기적 (CPU 명령어 실행과 무관하게 발생) | 동기적 (CPU 명령어 실행 시 발생) |
| **예시**    | 키보드, 마우스, 타이머, 네트워크 인터럽트   | 시스템 호출, 예외 처리         |
| **사용 목적** | 하드웨어 장치의 요청을 처리            | 운영체제와 프로그램 간 상호작용 처리  |
HW 인터럽트 
1. 외부 인터럽트 : 입출력 장치, 타이밍 장치, 전원 등 외부적인 요인
2. 내부 인터럽트 : Exception -> 잘못된 명령이나 데이터를 사용할 때 발생 (Division By Zero, Overflow/underflow, 기타 Exception
 SW 인터럽트
SVC(SuperVisor Call) 인터럽트
프로그램 처리 중 명령의 요청에 의해 발생한 것
## 동시에 두 개 이상의 인터럽트가 발생하면, 어떻게 처리해야 하나요?

**전원 이상 > 기계 착오 > 외부 신호 > 입출력 > 명령어 잘못 > 프로그램 검사 > SVC** 순서로 우선순위가 보통 적용된다. 우선순위에 따라 인터럽트를 실행한다.

# 3. 프로세스가 무엇인가요?
## 프로그램과 프로세스, 스레드의 차이에 대해 설명해 주세요.
**프로그램** 
컴퓨터가 실행할 수 있는 명령어들의 집합
**프로세스** 
컴퓨터에서 실행 중인 프로그램, 각각의 프로세스는 독립된 메모리 공간 할당
명령어들 데이터를 가진다.
- Code : 코드 자체를 구성하는 메모리 영역
- Data : 전역변수, 정적변수, 배열 등
- Heap : 동적 할당 시 사용
- Stack : 지역변수, 매개변수, 리턴 값
**스레드**
프로세스 내에서 실행되는 여러 흐름의 단위
- 프로세스 내 스레드는 메모리를 공유할 수 있다.

## PCB가 무엇인가요?
빠르게 번갈아 수행되는 프로세스들을 관리해야하는데 이를 위해 사용하는 자료구조가 프로세스 제어블록(PCB) 이다. 프로세스 생성 시 커널 영역에 생성되고 종료 시 폐기 된다.

- **프로세스 ID(=PID)** 
  특정 프로세스를 식별하기 위해 부여되는 고유한 번호
- **레지스터 값** 
  프로세스는 자신의 실행 차례가 오면 이전까지 사용한 레지스터 중간 값을 모두 복원
- **프로세스 상태**
  입출력 장치를 사용하기 위해 기다리는 상태
- **CPU 스케줄링 정보**
- **메모리 정보**
  프로세스가 현재 어느 주소에 저장되어 있는지에 대한 정보
- **사용한 파일과 입출력장치 정보**
  할당된 입출력 장치

## 그렇다면, 스레드는 PCB를 갖고 있을까요?
PCB와 조금 다른 용도와 구조를 갖는 TCB가 있다.
- 스레드 ID
- 스레드 상태
- 레지스터 값
- 스케줄링 정보 

## 리눅스에서, 프로세스와 스레드는 각각 어떻게 생성될까요?
프로세스는 fork() 시스템 콜을 통해서 생성되고, 스레드는 pthread_create()를 사용해서 새로운 스레드를 생성한다.
## 자식 프로세스가 상태를 알리지 않고 죽거나, 부모 프로세스가 먼저 죽게 되면 어떻게 처리하나요?
**자식 프로세스가 상태를 알리지 않고 죽는 경우**: 부모 프로세스는 wait()나 waitpid()를 사용해 자식의 종료 상태를 회수해야 합니다. 회수하지 않으면 좀비 프로세스로 남게 됩니다.
**부모 프로세스가 먼저 죽는 경우**: 고아 프로세스는 init 프로세스에 의해 관리되며, 운영체제가 자동으로 처리합니다.
## 리눅스에서, 데몬프로세스에 대해 설명해 주세요.
프로세스에는 포그라운드 프로세스, 백그라운드 프로세스가 있습니다. 포그라운드 프로세스는 사용자가 볼 수 있는 공간에서 실행되는 프로세스를 말하고, 웹 브라우저 같은 것을 말합니다. 백그라운드 프로세스는 사용자가 볼 수 없는 공간에서 실행되는 프로세스를 말합니다.
그 중 사용자와 직접 상호작용하지 않고 정해진 일만 수행하는 프로세스를 데몬 프로세스라고 합니다
## 리눅스는 프로세스가 일종의 트리를 형성하고 있습니다. 이 트리의 루트 노드에 위치하는 프로세스에 대해 설명해 주세요.
루트 노드는 init 프로세스이다. init 프로세스는 시스템 부팅 시 가장 먼저 실행되는 프로세스로, 나머지 모든 프로세스는 init 프로세스의 자손들이다.

1. 프로세스 관리 - 프로세스 생성과 초기화 그리고 종료를 정리하는 역할
2. 데몬 프로세스 관리
3. 고아 프로세스 관리
- 어떤 인터럽트 요청을 거절하는 경우가 어떤경우가 있나요?
  * 우선순위에 따라 거절을 하기도 하나요?
* 인터럽트가 많이 쌓였을 경우 인터럽트를 어떻게 관리하나요? (큐로 생각하긴 했지만 정확하게 모르겠어서 질문합니다)

# 4. 프로세스 주소공간에 대해 설명해 주세요.

<img width="325" alt="스크린샷 2024-10-30 오후 2 51 06" src="https://github.com/user-attachments/assets/ce8b4cbe-9696-4440-9999-d7eec8ca6689" />

**텍스트** 
- 실행 코드 
- Read-Only로 지정
- 컴파일시에 결정된다 (.java -> .class)

**데이터** 
- 전역 변수, 정적 변수(static) 가 저장되는 곳
- 프로그램 시작과 함께 할당되고 종료되면 소멸된다.
- 컴파일시에 결정된다.
- 초기화된 데이터를 데이터 영역, 초기화되지 않은 데이터는 BSS(Block Stated Symbol) 영역에 저장된다. (`static int a;` 로 선언하면 a 는 BSS 영역에 할당)
- Read-Write로 지정

**힙** 
- 프로그램 실행 중에 동적으로 할당되는 메모리
- 런타임 시 크기가 결정된다.
- 낮은 주소 -> 높은 주소

**스택** 
- 함수를 호출할 때 임시 데이터 저장장소(함수 매개변수, 복귀 주소 및 지역 변수)
- 컴파일 시 크기가 결정된다.
- 높은 주소 -> 낮은 주소

- 텍스트, 데이터 공간은 크기가 고정되어 있어 프로그램 실행 시간 동안 크기가 변하지 않는다.
- 스택, 힙은 프로그램 실행 중에 동적으로 줄어들거나 커질 수 있다.
  - 함수가 호출될 때마다 함수 매개변수, 지역 변수, 및 복귀 주소를 포함하는 활성화 레코드가 푸쉬되고, 함수에서 제어가 되돌아오면 스택에서 활성화 레코드가 팝된다.
  - 메모리가 동적으로 할당됨에 따라 힙이 커지고 메모리가 시스템에 반환되면 작아진다.

## 초기화 하지 않은 변수들은 어디에 저장될까요?
BSS 영역에 저장해둔다.

**data 영역과 BSS 영역을 구분하는 이유?**
> 초기화가 되지 않는 변수는 프로그램이 실행될때 영역만 잡아주면 되고 그 값을 프로그램에 저장하고 있을 필요는 없으나 초기화가 되는 변수는 그 값도 프로그램에 저장하고 있어야 하기때문에 두가지를 구분해서 영역을 잡는것이다. 이것이 bss영역을 구분하는 이유이다. 따라서 이러한 bss영역 변수들은 많아져도 프로그램의 실행코드 사이즈를 늘리지 않는다.

## 일반적인 주소공간 그림처럼, Stack과 Heap의 크기는 매우 크다고 할 수 있을까요? 그렇지 않다면, 그 크기는 언제 결정될까요?
Stack 은 컴파일 시 크기가 결정되고 Heap은 런타임 시 크기가 결정된다. 
- 할당된 Stack 크기를 넘어가면 -> Overflow
- 최대 Heap 의 크기를 넘어가면 -> OutOfMemory
## Stack과 Heap 공간에 대해, 접근 속도가 더 빠른 공간은 어디일까요?
Stack 이 더 빠르다. Stack 영역은 이미 생성된 공간에 스택 포인터의 위치만 바꿔주는 간단한 CPU 연산이다.
Heap 영역에서 메모리 할당은 요청된 메모리의 크기, 현재 메모리의 fragmentation 상황 등 고려해야할 요소가 더 많다.

## 다음과 같이 공간을 분할하는 이유가 있을까요?
효율적인 메모리 관리를 하기 위해서이다.
예를 들어 각각의 스레드는 Stack 영역을 갖고 있지만 Data영역은 공유한다. 똑같은 공간을 여러개 만들지 않고 메모리를 절약할 수 있다.
## 스레드의 주소공간은 어떻게 구성되어 있을까요?

![pt_2](https://github.com/user-attachments/assets/ca1c7f88-2686-4394-a337-3fe6b5958fc9)

스레드는 프로세스의  텍스트, 데이터, BSS, 힙 영역을 모두 공유하고, 개별적으로 스택 영역과 PC 레지스터 값을 가진다.

## "스택"영역과 "힙"영역은 정말 자료구조의 스택/힙과 연관이 있는 걸까요? 만약 그렇다면, 각 주소공간의 동작과정과 연계해서 설명해 주세요.
스택 메모리의 동작
- 함수 호출 시 : 스택 프레임이 생성되고, 함수의 매개변수, 복귀 주소, 지역 변수가 스택에 쌓인다.
- 함수 종료 시 : 해당 스택 프레임이 제거되면서 스택 메모리에서 제거된다.
-> 함수 호출과 반환이 자료구조 스택처럼 가장 최근에 호출된 함수가 먼저 종료 되어 메모리에서 제거된다.

힙 메모리의 동작
- 프로그램이 실행 중에 동적으로 new 또는 malloc 등을 통해 메모리를 할당하면 힙 영역에서 메모리가 할당된다.
- 힙 메모리는 임의의 크기로 할당할 수 있어 스택처럼 순차적이지 않고, 동적 메모리 관리자가 메모리를 적절히 할당하고 해제한다.
- 동적으로 할당된 메모리의 생명 주기는 프로그래머가 직접 관리하며, 사용이 끝나면 명시적으로 해제해야 한다.
-> 힙 영역은 자료구조 힙과 큰 연관이 없다.
## IPC의 Shared Memory 기법은 프로세스 주소공간의 어디에 들어가나요? 그런 이유가 있을까요?
공유 메모리 영역은 기존에 있는 메모리 영역이 아닌 별도의 공간에 할당된다.
별도의 영역에 할당되는 이유는 여러 프로세스가 그 공유 메모리에 접근해야 한다. 프로세스의 독립적인 공간인 데이터, 힙 등에 들어가게된다면 둘 이상의 프로세스가 그 공유 메모리를 가지고 통신을 할 수 없다.
## 스택과 힙영역의 크기는 언제 결정되나요? 프로그램 개발자가 아닌, 사용자가 이 공간의 크기를 수정할 수 있나요?
스택은 컴파일 시점에 결정되고 힙은 런타임 시점에 크기가 결정된다.

일반적인 사용자가 스택과 힙의 크기를 수정하는 것은 불가능하다. 하지만 고급 사용자나 관리자는 운영체제의 설정을 통해 이러한 값들을 변경할 수 있을 수 있다. 예를 들어 Linux에서는 ‘ulimit’ 명령을 사용해 프로세스의 스택 크기를 조정할 수 있다.

#  5. 단기, 중기, 장기 스케쥴러에 대해 설명해 주세요.

<img width="721" alt="스크린샷 2024-10-31 오후 12 53 41" src="https://github.com/user-attachments/assets/4bb9c496-ff84-495e-8ff7-360e9885ea7e" />

**단기 스케줄러**
- CPU 스케줄러라고도 함
- 준비 상태의 프로세스 중 어떤 프로세스를 다음 번에 실행 상태로 만들 것인지 결정
- 타이머 인터럽트가 발생하면 단기 스케줄러 호출
  - 타이머 인터럽트 : 시스템 시간을 정기적으로 업데이트하고, 프로세스가 할당된 CPU 시간을 제한하는 등에 사용됨
- 미리 정한 스케줄링 알고리즘에 따라 CPU를 할당 할 프로세스를 선택함
- 매우 짧은 시간으로 빈번하게 호출되기 때문에 수행 속도가 매우 빨라야 함

**중기 스케줄러**
- 너무 많은 프로세스에게 메모리를 할당해 시스템의 성능이 저하되는 경우 이를 해결하기 위해 메모리에 적재된 프로세스의 수를 동적으로 조절하기 위한 스케줄러
- 만약 메모리에 많은 수의 프로세스가 적재되어 프로세스 당 보유하고 있는 메모리량이 극도록 적어지게 되면 CPU 수행에 당장 필요한 프로세스의 주소 공간조차도 메모리에 올려놓기 어려운 상황이 발생함(디스크 I/O가 수시로 발생히 시스템 성능 저하 우려)
- 이런 경우 메모리에 올라와 있는 프로세스 중 일부를 메모리를 통째로 빼앗아 그 내용을 디스크의 스왑 영역에 저장해둠(Swap out)

**장기 스케줄러**
- 작업 스케줄러라고도 함
- 어떤 프로세스를 준비 큐에 삽입할지 결정하는 역할을 함
- 디스크에서 하나의 프로그램을 가져와 커널에 등록하면 프로세스가 되는데, 이 때 디스크에서 어떤 프로그램을 가져와 커널에 등록할 지(준비 큐에 등록할 지) 결정
- 장기 스케줄러는 수십 초 내지 분 단위로 가끔 호출되기 때문에 상대적으로 속도가 느린 것이 허용됨
- 메모리에 동시에 올라가 있는 프로세스의 수를 조절하는 역할을 함

## 현대 OS에는 단기, 중기, 장기 스케쥴러를 모두 사용하고 있나요?
현대 OS는 시분할 시스템을 사용하여 장기 스케줄러는 대부분 사용하지 않는다.
과거에는 적은 양의 메모리를 많은 프로세스들에게 할당하게 되면 프로세스 당 메모리 보유량이 적어져 장기 스케줄러가 이를 관리하는 역할을 했지만, 현대의 OS는 프로세스가 시작되면 장기 스케줄러 없이 바로 프로세스를 메모리에 할당해 준비 큐에 넣어준다.

## 프로세스의 스케쥴링 상태에 대해 설명해 주세요.

- new : 프로세스가 생성된 상태. 아직 할당되지 않은 자원을 갖고 있다.
- running : 프로세스가 CPU를 사용하며 실행되는 상태
- blocked(waiting) : 프로세스가 어떤 이벤트가 일어나기를 기다리는 상태
- ready :  프로세스가 실행을 기다리는 상태
- terminated : 프로세스가 완전히 종료된 상태
- suspended : 프로세스의 중지 상태

## preemptive/non-preemptive 에서 존재할 수 없는 상태가 있을까요?
**preemptive**
어떤 프로세스가 CPU를 할당받아 실행 중에 있어도 다른 프로세스가 실행 중인 프로세스를 중지하고 CPU를 가제로 공유할 수 있다.

**non-preemptive**
어떤 프로세스가 CPU를 할당 받으면 그 프로세스가 종료되거나 I/O 요청이 발생하여 자발적으로 중지될 때까지 계속 실행되도록 보장한다.
- Running -> Ready 상태로 전환되는 것이 발생하지 않는다.
## Memory가 부족할 경우, Process는 어떠한 상태로 변화할까요?
대기중인 프로세스가 할당된 메모리를 반납하고 하드 디스크로 Swap-out 된다. (suspended 상태)
이 후 Memory 여유가 생기면 다시 메모리 Swap-in 한다.

# 6. 컨텍스트 스위칭 시에는 어떤 일들이 일어나나요?
**컨텍스트 스위칭**
CPU를 한 프로세스에서 다른 프로세스로 넘겨주는 과정

<img width="457" alt="스크린샷 2024-10-31 오후 2 45 05" src="https://github.com/user-attachments/assets/f555b43b-f833-4b9d-803f-8d9f9468caf6" />

- CPU를 내어주는 프로세스의 상태를 그 프로세스의 PCB에 저장
- CPU를 새롭게 얻는 프로세스의 상태를 PCB에서 읽어옴

## 프로세스와 스레드는 컨텍스트 스위칭이 발생했을 때 어떤 차이가 있을까요?
컨텍스트 스위칭이 수행하는 작업들
1. 현재 실행 중인 프로세스 혹은 스레드의 컨텍스트 백업(CPU 레지스터 값들, 어디까지 실행됐는지 등)
2. CPU 캐시를 비움(flush), CPU마다 L1, L2 캐시에 대한 동작이 다를 수 있음, 안 비울 수도 있음
3. TLB를 비움
4. MMU를 변경
프로세스 컨텍스트 스위칭은 위 4개를 모두 수행
스레드 컨텍스트 스위칭은 1 번만 수행

서로 다른 프로세스는 다른 메모리 주소 공간을 가지지만, 같은 프로세스에 속한 스레드는 프로세스의 메모리 주소 공간을 공유하기 때문이다.

따라서 스레드 컨텍스트 스위칭에 속도가 더 빠르고 가볍다.

## 컨텍스트 스위칭이 발생할 때, 기존의 프로세스 정보는 커널스택에 어떠한 형식으로 저장되나요?
**Kernal Address Space**

![299152921-c8b2719b-556b-4224-9d7f-08905ec627d3](https://github.com/user-attachments/assets/8e7a9fa1-51b2-43ef-896e-4c9d07fe6ebd)

커널 스택은 각 프로세스나 스레드가 커널 모드에서 실행될 때 사용되는 스택이며, 이 스택에는 중요한 상태 정보가 저장된다.

PCB라는 자료구조에 현재 레지스터 상태, PC 값, 스택 포인터(SP) 등이 저장된다.

## 컨텍스트 스위칭은 언제 일어날까요?
1. Timer Interrupt 발생하여 다른 프로세스로 넘어갈 때
2. 프로세스 스케줄링으로 인한 프로세스 변경될 때
3. I/O 요청 (?)
4. 시스템 콜, 인터럽트 (?)

<img width="586" alt="스크린샷 2024-10-31 오후 3 14 26" src="https://github.com/user-attachments/assets/5fdc2805-eac9-4a81-9b60-2a461420986d" />

#  7. 프로세스 스케줄링 알고리즘에는 어떤 것들이 있나요?
**FCFS(선입 선처리)**
- 준비 큐에 삽입한 순서대로 처리하는 비선점형
- 프로세스들이 기다리는 시간이 매우 길어질 수 있다(=호위 효과 convoy effect)

**SJF(최단 작업 우선)**
- CPU 사용이 짧은 프로세스를 먼저 실행 -> 호위 효과 방지

**RR(라운드 로빈)**
- FCFS + Time slice(각 프로세스가 CPU를 사용할 수 있는 시간)
- 정해진 Time slice만큼 CPU를 돌아가며 이용하는 선점형

**SRT(최소 잔여 시간 우선)**
- SJF + RR
- 정해진 Time slice만큼 CPU를 이용하고, 다음 CPU를 사용할 프로세스는 남은 작업 시간이 가장 적은 프로세스를 선택

**우선순위 스케줄링**
- 프로세스들에게 우선순위 부여하고, 우선순위가 높은 프로세스부터 실행
- 우선순위가 같으면 FCFS 방식으로 처리
- 우선순위가 낮은 프로세스는 실행되는데 오래 걸림(=기아현상 starvation)
  - 에이징(aging) : 오랫동안 대기하는 프로세스의 우선순위를 점진적으로 증가시킴

**Multi-Level Queue(다단계 큐)**
- 우선순위 별로 준비 큐를 여러 개 사용하는 스케줄링 방식
  - 우선순위가 가장 높은 큐에 있는 프로세스를 먼저 처리
  - 우선순위가 가장 높은 큐가 비어있으면 그 다음 우선순위 큐에 있는 프로세스 처리

**Multi-Level Feedback Queue(다단계 피드백 큐)**
- 큐 간의 이동이 가능한 다단계 큐 스케줄링
- CPU 시간을 많이 사용하는 프로세스는 낮은 우선순위의 큐로, I/O 집중 프로세스의 우선순위는 높은 큐로 이동
- 에이징 기법 사용

## RR을 사용할 때, Time Slice에 따른 trade-off를 설명해 주세요.
Time Slice가 너무 짧으면 -> 컨텍스트 스위칭이 자주 일어나기 때문에 비효율적이다.
Time Slice가 너무 길면 -> 호위 효과가 생긴다.

## 싱글 스레드 CPU 에서 상시로 돌아가야 하는 프로세스가 있다면, 어떤 스케쥴링 알고리즘을 사용하는 것이 좋을까요? 또 왜 그럴까요?
선점형 스케줄링을 선택해야한다. 비선점형 스케줄링을 선택할 시 상시로 돌아가야하는 프로세스가 오랜시간 CPU를 잡아먹는 프로세스라면 다른 프로세스가 실행되는데 너무 오래 기다릴 수 있다.
**MLFQ** 스케줄링으로 우선순위가 높은 프로세스부터 실행할 수 있도록 한다.

> 싱글 스레드 CPU 에서 상시로 돌아가야 하는 프로세스가 있다면, 어떤 스케쥴링 알고리즘을 사용하는 것이 좋을까요? 또 왜 그럴까요?
> 여기서 말하는 상시로 돌아가는 프로세스의 의미를 주기적으로 계속 돌아가는 프로세스라고 생각을 하여서, 선점형 스케줄링을 선택해야 한다고 생각했습니다.

근데 선점형 스케줄링에서 어떤 스케줄링 알고리즘을 선택해야하는지에 대한 기준을 정하는게 어려운 것 같습니다.
## 동시성과 병렬성의 차이에 대해 설명해 주세요.

**동시성**
하나의 시스템이 여러 작업을 동시에 처리하는 것처럼 보이게 하는 것
실질적으로 한번에 하나의 작업만 처리
 
**병렬성**
여러 작업을 실제로 동시에 처리하는 것
멀티 코어를 사용하면 여러 작업을 병렬로 처리할 수 있다.

## 타 스케쥴러와 비교하여, Multi-level Feedback Queue는 어떤 문제점들을 해결한다고 볼 수 있을까요?

**MLFQ 의 규칙**
- 규칙 1. 우선권(우선순위)이 높은 작업을 먼저 실행
- 규칙 2. 같은 우선권을 가지는 작업은 RR 적용
- 규칙 3. 작업이 생성되면 가장 높은 우선권을 부여(최상단 큐에 배치)
- 규칙 4. CPU를 포기한 횟수에 상관없이 프로세스가 큐에 할당된 CPU 시간을 소진하면 우선권을 낮춤(하위 큐로 이동)
  - 규칙 4a. 작업이 주어진 타임 슬라이스를 다 소진하면 우선권 내림(하위 큐로 이동)
  - 규칙 4b. 작업이 타임 슬라이스를 소진 못하고 포기하면 우선권 유지(해당 큐 유지)
- 규칙 5. 일정 기간 s가 지나면 모든 작업에 우선권 부여(최상위 큐에 포함)

규칙 5를 통해 Starvation을 해결

**s의 값을 잘 정하는 것이 중요하다.**
- Too high -> Long running process가 손해
- Too low -> interactive process가 손해


## FIFO 스케쥴러는 정말 쓸모가 없는 친구일까요? 어떤 시나리오에 사용하면 좋을까요?
우선순위가 같은 여러 프로세스를 처리해야할 때 사용한다. 

## 우리는 스케줄링 알고리즘을 "프로세스" 스케줄링 알고리즘이라고 부릅니다. 스레드는 다른 방식으로 스케줄링을 하나요?
프로세스 스케줄링 : 서로 다른 프로세스 내에 있는 스레드들끼리의 스케줄링
스레드 스케줄링 : 한 프로세스 내에 있는 스레드들끼리의 스케줄링

## 유저 스레드와 커널 스레드의 스케쥴링 알고리즘은 똑같을까요?

# 8. 뮤텍스와 세마포어의 차이점은 무엇인가요?
**뮤텍스**
상호 배제를 위한 동기화 도구
- 자물쇠 역할 : 프로세스들이 공유하는 전역 변수 lock
- 임계 구역을 잠그는 역할 : acquire 함수
- 임계 구역의 잠금을 해제하는 역할 : release 함수

**세마포어**
좀 더 일반화된 동기화 도구, 공유 자원이 여러 개 있는 경우에도 적용 가능.
임계 구역 앞에서 멈춤 신호를 받으면 잠시 기다리기
임계 구역 앞에서 가도 좋다는 신호를 받으면 임계 구역 진입
- 임계 구역에 진입할 수 있는 프로세스의 개수를 나타내는 전역 변수 S
- 임계 구역에 들어가도 좋은지 기다려야 할지를 알려주는 wait 함수
- 임계 구역 앞에서 기다리는 프로세스에 이제 가도 좋다고 신호를 주는 signal 함수

**차이점**
1. 임계 구역에 들어갈 수 있는 프로세스의 개수 차이
   - 뮤텍스는 1개, 세마포어는 여러개
## 이진 세마포어와 뮤텍스의 차이에 대해 설명해 주세요.
1. 뮤텍스는 락을 가진 자만이 락을 해제할 수 있다. 이진 세마포어는 다른 프로세스/스레드가 락을 해제할 수 있다.
2. 뮤텍스는 Priority inheritance 속성을 가지지만 이진 세마포어는 그 속성이 없다.

**Priority inheritance**
> Lock을 획득한 프로세스 이후에 높은 프로세스가 존재한다면 Lock을 획득한 프로세스의 우선순위를 높여 작업을 빠르게 끝내는 것
> 뮤텍스가 가능한 이유는 락을 획득한 프로세스만이 락을 해제할 수 있고, 세마포어는 누가 락을 해제할지 모르기 때문이다.

## Lock을 얻기 위해 대기하는 프로세스들은 Spin Lock 기법을 사용할 수 있습니다. 이 방법의 장단점은 무엇인가요? 단점을 해결할 방법은 없을까요?
**Sping Lock** 
락을 얻기 위해 대기하는 동안 스레드가 CPU 사이클을 소모하며 계속해서 락을 시도하는 방법

**장점**
- 락이 짧은 시간 내에 해제될 경우 빠르게 락을 얻을 수 있다.
- 스핀 락은 스레드를 블록하지 않아 컨텍스트 스위칭 오버헤드가 없다.
- 구현이 간단하다.

**단점**
- CPU 사이클을 계속 소모하기 때문에 락을 얻는 시간이 오래걸릴 경우 CPU 자원을 낭비하게 된다.
- 우선순위가 낮은 스레드가 락을 가지고 있으면, 우선순위가 높은 스레드도 락을 얻기 위해 계속 대기해야 한다. -> 우선순위 역전 현상

**해결 방법**
해결 방법으로는 임계 구역에 접근할 수 없으면 프로세스의 PCB을 대기 큐에 넣어 대기 상태로 만든다. 그러다가 임계 구역에 접근할 수 있게되면 대기 큐에서 꺼내 프로세스를 준비 큐에 넣고 준비 상태로 만든다.

> 유저레벨에서 선택한 락이 OS로 넘어가서도 동작하는지

## 뮤텍스와 세마포어 모두 커널이 관리하기 때문에, Lock을 얻고 방출하는 과정에서 시스템 콜을 호출해야 합니다. 이 방법의 장단점이 있을까요? 단점을 해결할 수 있는 방법은 없을까요?
**장점**
1. 커널이 Lock을 관리하기 때문에 프로세스 간에 안전한 동기화가 보장된다.
2. 커널이 리소스를 관리하므로 여러 프로세스 간의 동기화에 대한 처리를 효율적으로 수행할 수 있다.

**단점**
1. 시스템 콜 오버헤드가 발생할 수 있다.
2. 빈번한 락의 획득가 해제가 성능 저하가 될 수 있다.

# 9. Deadlock 에 대해 설명해 주세요.
## Deadlock 이 동작하기 위한 4가지 조건에 대해 설명해 주세요.
1. 상호 배제 : 자원은 한 번에 하나의 프로세스만 사용 가능하다.
2. 점유 대기 : 최소한 하나의 자원을 점유하고 있으면서 다른 프로세스에 할당되어 사용하고 있는 자원을 추가로 점유하기 위해 대기하는 프로세스가 존재해야 한다.
3. 비선점 : 다른 프로세스에게 할당된 자원은 사용이 끝날 때까지 강제로 빼앗을 수 없다.
4. 순환 대기 : 프로세스의 집합에서 순환 형태로 자원을 대기하고 있어야 한다.
## 그렇다면 3가지만 충족하면 왜 Deadlock 이 발생하지 않을까요?
4개 조건에서 1개 조건만 충족을 못하게 되면 교착 상태가 발생하지 않는다.
- **상호 배제 -> 비상호 배제** : 한 번에 여러개 프로세스가 자원을 사용할 수 있으니 대기 상태가 안 일어난다.
- **점유 대기 -> 비점유 대기** : 자원을 점유하고 있지 않으므로 다른 프로세스가 무한히 대기하는 상황이 안 일어난다.
- **비선점 -> 선점** : 프로세스의 자원을 뺏어서 사용할 수 있으므로 무한 대기가 안 일어난다.
- **순환 대기 -> 비순환 대기** : 순환구조가 아니면 하나의 자원에 대한 락이 해제되면 순차적으로 작업을 진행할 수 있다.
## 어떤 방식으로 예방할 수 있을까요?
**예방**
교착 상태 발생 조건 중 하나를 제거하여 교착 상태를 예방하는 방법
- 상호 배제를 없애서 예방한다.
  - 이론적으로 가능하나 현실적으로는 힘든 방법이다.
- 점유와 대기를 없애서 예방한다.
  - 특정 프로세스에 자원을 모두 할당하거나, 아예 할당하지 않는 방식으로 배분
  - 단점으로 자원의 활용률을 낮출 수 있는 방법이다.
- 비선점 조건을 없애서 예방한다.
  - 한 프로세스가 다른 프로세스의 자원을 뺏는다.
  - 선점이 가능한 자원에 한해 효과적(CPU) 하지만 모든 자원이 선점 가능한 것은 아니다.
- 순환 대기 조건을 없애서 예방한다.
  - 자원에 번호를 붙이고 오름차순으로 할당하면 원형 대기는 발생하지 않는다.
  - 자원에 번호 붙이는 것은 어려운 작업이고 어떤 자원에 어떤 번호를 붙이느냐에 따라 활용률이 달라진다.
**회피**
교착 상태가 발생하지 않을 정도로만 조심스럽게 자원을 할당하는 방식
- 안전 상태에서 안전 상태로 움직이는 경우에만 자원을 할당하는 방식
- 은행원 알고리즘

**탐지 후 회복**
교착 상태 발생을 인정하고 사후에 조치하는 방식
- 선점을 통한 회복
  - 교착 상태가 해결될 때까지 한 프로세스씩 자원을 몰아주는 방식
- 프로세스 강제 종료를 통한 회복
  - 교착 상태에 놓인 프로세스 모두 강제 종료( -> 작업 내역을 잃을 위험)
  - 교착 상태가 해결될 때까지 한 프로세스씩 강제 종료( -> 오버헤드)
  
## 왜 현대 OS는 Deadlock을 처리하지 않을까요?
현대 대부분 OS는 문제를 무시하고, 교착 상태가 시스템에서 절대 발생하지 않는 척한다.

위에서 말한 방법으로 데드락 문제를 다루는 것이 오히려 추가적인 비용과 성능 저하를 일으키기 때문이다.
- 상호 배제가 없는 경우엔 자원을 프로세스가 공유하기 때문에 문제가 발생한다.
- 점유 대기가 없는 경우 특정 프로세스에 자원을 모두 할당하거나 아예 할당하지 않아야하는데 자원의 활용률이 낮아진다.
- 선점형으로 할 경우, 선점이 가능한 자원에 한해 효과적이지만 모든 자원이 선점 가능한 것이 아니다.
- 순환 대기를 없애는 방법은 자원의 번호를 붙여야 하는데 이 작업이 어렵다.

## Wait Free와 Lock Free를 비교해 주세요.

**Lock-free** 
여러 개의 스레드에서 동시에 호출했을 때에도 정해진 단위 시간마다 적어도 한 개의 호출이 완료되는 알고리즘
멀티스레드 환경에서 다른 스레드가 플래그를 세팅해주고, lock을 풀어 주는 등 다른 스레드가 끝나고 자기 순서가 오기를 기다리지 않는 논 블로킹이 보장되어야 Lock-free가 될 수 있다.

**장점**
- 성능이 좋다.
- 높은 부하에도 안정적이다.
**단점**
- 알고리즘이 복잡해진다.
- 메모리 재사용이 어렵다 ABA문제가 발생한다.

**ABA 문제**
> CAS를 사용해서 자료구조의 아이템을 변경할 때, 포인터가 시스템에 의해 재사용되면서 생기는 문제

**Wait free**
여러 스레드가 하나의 작업을 동시에 호출했을 때 지연 없이 모든 스레드가 동시에 작업을 완료할 수 있는 알고리즘

# 10. 프로그램이 컴파일 되어, 실행되는 과정을 간략하게 설명해 주세요.
1. 소스 코드 작성 
   - 개발자가 프로그래밍 언어로 소스 코드를 작성
2. 전처리 과정 
   - 매크로를 확장하고 주석을 제거
3. 컴파일 
   - 소스 코드를 기계어에 가까운 중간 코드나 어셈블리어로 변환
   - 문법 검사를 수행하고 오류가 있으면 알려준다.
4. 어셈블 
   - 어셈블리어를 기계어로 변환
   - 목적 코드 생성
5. 링킹 
   - 여러 개의 목적 파일을 하나의 실행 파일로 결합
   - 라이브러리 함수들을 연결, 최종 실행 파일 생성
6. 로딩 
   - 운영 체제가 실행 파일을 메모리에 로드
   - 프로그램 실행에 필요한 메모리 공간 할당

## 링커와, 로더의 차이에 대해 설명해 주세요.
링커 : 독립적으로 컴파일된 여러 코드 모듈을 하나의 실행 파일로 합치는 역할을 한다.
로더 : 실행 파일을 실제 메모리에 적재하여 실행할 수 있도록 준비하는 역할을 한다.
## 컴파일 언어와 인터프리터 언어의 차이에 대해 설명해 주세요.
**컴파일 언어(C, C++)** 
- 소스코드 전체를 기계어로 번역 후 실행
- 실행 파일 생성 필요
- 실행 속도가 빠르다
- 코드 수정 후 실행하려면 다시 컴파일해야 한다.

**인터프리터 언어(Python, JavaScript)**
- 소스코드를 한 줄씩 해석하며 바로 실행
- 별도의 실행 파일 불필요
- 코드를 작성하고 곧바로 실행할 수 있어 디버깅과 테스트가 용이하다.
- 실행할 때마다 소스 코드를 해석해야 하므로 실행 속도가 느릴 수 있다.

## JIT에 대해 설명해 주세요.

**JIT 컴파일러 (Just-In-Time Compiler)**
인터프리터의 단점을 보완하기 위해 도입된 방식으로 바이트 코드 전체를 컴파일하여 바이너리 코드로 변경하고 이후에는 해당 메서드를 더 이상 인터프리팅 하지 않고, 바이너리 코드로 직접 실행하는 방식이다. 


**JIT 컴파일러의 동작 방식**
1. 바이트코드 컴파일 : 프로그램이 실행되기 전에 소스 코드는 먼저 바이트코드(중간 코드)로 변환됩니다. Java의 경우, 소스 코드는 먼저 Javac 컴파일러에 의해 바이트코드로 컴파일되고, 이후 JVM(Java Virtual Machine)에서 실행됩니다.
2. 실행 중 컴파일 : JVM은 바이트코드를 한 줄씩 해석하며 실행합니다. 그러다 특정 코드가 반복적으로 실행되면, JIT 컴파일러는 해당 바이트코드를 즉시 기계어로 변환합니다.
3. 캐싱 : JIT 컴파일러가 기계어로 컴파일한 코드(네이티브 코드)는 캐시에 저장되며, 이후 동일한 코드가 실행될 때마다 다시 해석할 필요 없이 기계어로 실행되어 속도가 크게 향상됩니다.

**장점**
- 인터프리터 방식보다 성능이 크게 향상된다.
**단점**
- 메모리 사용량이 증가한다.

## 본인이 사용하는 언어는, 어떤식으로 컴파일 및 실행되는지 설명해 주세요.

![img_java_programming](https://github.com/user-attachments/assets/07660e1d-9f38-4e3b-9f07-30c462f8b5cb)
![991D064B5AE999D512](https://github.com/user-attachments/assets/cc16b1de-2ff2-406d-b31c-d1deddc9e7e3)

1. 개발자가 자바 소스코드(.java)를 작성한다.
2. 자바 컴파일러가 자바 소스파일을 컴파일한다. 자바 소스코드(.java) -> 자바 바이트 코드(.class) 아직 컴퓨터가 읽을 수 없는 JVM이 이해할 수 있는 코드이다. 
3. 컴파일된 바이트 코드를 JVM의 클래스로더에게 전달한다.
4. 클래스 로더(링커 역할)는 동적 로딩을 통해 필요한 클래스들을 로딩 및 링크하여 런타임 데이터 영역, JVM 메모리에 올린다.
5. 실행 엔진은 JVM 메모리에 올라온 바이트 코드들을 명령어 단위로 하나씩 가져와서 실행한다.
   1. 인터프리터
   2. JIT 컴파일러
## Python 같은 언어는 CPython, Jython, PyPy등의 다양한 구현체가 있습니다. 각각은 어떤 차이가 있을까요? 또한, 실행되는 과정 또한 다를까요?
 **CPython**
- 소스코드 -> 바이트코드
- Python 소스 코드를 읽고 이를 바이트 코드(.pyc)로 컴파일한다. 
- CPython 인터프리터는 생성된 바이트 코드를 순차적으로 읽고 해석하여 실행한다.

**Jython**
- 소스코드 -> Java 바이트코드
- 생성된 Java 바이트코드는 JVM에서 실행된다. 
- Java 라이브러리를 직접 사용할 수 있는 장점이 있고, Java와 상호 운용성이 좋다.

**PyPy**
- 소스코드 -> 바이트코드
- PyPy 자체 인터프리터에 맞춰 최적화되어 있다.
- JIT 컴파일러를 사용해서 실행 중에 자주 호출되는 코드를 감지하여 이 코드를 기계어로 컴파일한다.
## 우리는 흔히 fork(), exec() 시스템 콜을 사용하여 프로세스를 적재할 수 있다고 배웠습니다. 로더의 역할은 이 시스템 콜과 상관있는 걸까요? 아니면 다른 방식으로 프로세스를 적재할 수 있는 건가요?

**fork()**
현재 프로세스를 복제하여 새로운 프로세스를 생성한다. 

**exec()**
현재 프로세스의 메모리 공간을 새로운 프로그램으로 교체한다. 현재 프로세스의 실행을 중지하고 새로운 프로그램을 실행하게 된다.
기존의 메모리 내용은 모두 삭제되고, 새로 로드된 프로그램의 코드와 데이터로 대체된다.

**로더의 역할**
운영체제에서 프로그램을 실행하기 위해 바이너리 파일을 메모리에 적재하는 기능을 담당한다.
커널이 `exec()` 시스템 콜을 호출할 때, 로더가 호출되어 지정된 바이너리 파일을 메모리에 로드한다.

**로더와 별도로 다른 방식으로 프로세스를 적재하는 방식**
- 초기 부팅 시에는 커널이 직접 프로그램을 메모리에 로드하는 방식을 사용한다.

# 11. IPC가 무엇이고, 어떤 종류가 있는지 설명해 주세요.
**IPC**
독립된 주소 공간에 있는 프로세스 간의 통신을 할 수 있게 하는 방법이다.

**메모리 공유 모델**
- 프로세스 특정 메모리 영역을 공유하는 모델
- 공유한 메모리 영역에 읽기/쓰기를 통해서 통신을 수행
- 프로그램 레벨에서 통신 기능 제공
  - 공유 메모리가 설정되면, 그 이후의 통신은 커널에 관여 없이 진행 가능하다.
- 구현 IPC - Shared Memory

**메모리 전달 모델**
- 고정길이 메시지, 가변길이 메시지를 송/수신자끼리 주고받음
- 커널을 통한 메시지 통신 기능을 제공
- 클라이언트 - 서버 방식의 통신
- 구현 IPC - PIPE, Message Queue, Socket

1. pipe
임시 큐를 통해 하나의 프로세스가 다른 프로세스로 데이터를 직접 전달하는 방법
- 데이터는 한 쪽 방향으로만 이동한다. 양방향 통신을 위해서는 두 개의 파이프가 필요
- 보낸 순서대로만 받음
- 익명 pipe
  - 부모-자식 프로세스 간 통신만 가능하다.
- 네임드 pipe
  - 외부 프로세스와 통신 가능하다.
2. 메시지 큐
고정된 크기를 갖는 메시지의 연결 리스트를 이용하여 통신을 하는 방법
- 메시지 단위의 통신, 메시지 큐 ID를 통해 통신
- 단방향도 지원하고 양방향도 지원한다.
3. 소켓
포트(Port)를 통해 통신
- 양방향 통신이 가능하다.
4. 공유 메모리
두 개 이상의 프로세스들이 하나의 메모리 영역을 공유하여 통신을 하는 방법
- 메모리의 직접 사용으로 빠르고 자유로운 통신이 가능
- 둘 이상의 프로세스가 동시에 메모리를 변경하지 않도록 프로세스 간의 동기화가 필요하다.
## Shared Memory가 무엇이며, 사용할 때 유의해야 할 점에 대해 설명해 주세요.

둘 이상의 프로세스가 동시에 메모리를 변경하지 않도록 프로세스 간 동기화가 필요하다. 뮤텍스, 세마포어 등의 동기화 도구가 있다.
## 메시지 큐는 단방향이라고 할 수 있나요?
일반적으로 단방향 통신 방식을 사용하고 양방향 통신을 하고 싶으면 두 개의 큐를 사용하면 된다.

프로세스 A,B,C 모두 생산자, 소비자가 될 수 있다고 한다면 하나의 큐에 데이터를 넣어 양방향 반이중으로 봐도되는지? 
# 12. Thread Safe 하다는 것은 어떤 의미인가요?
멀티 스레드 프로그래밍에서 일반적으로 어떤 함수나 변수, 혹은 객체가 여러 스레드로부터 동시에 접근이 이루어져도 프로그램의 실행에 문제가 없음을 뜻한다.
> 하나의 함수가 한 스레드로부터 호출되어 실행 중일 때, 다른 스레드가 그 함수를 호출하여 동시에 함께 실행되더라도 각 스레드에서의 함수의 수행 결과가 바로 나오는 것

## Thread Safe 를 보장하기 위해 어떤 방법을 사용할 수 있나요?
1. Mutual exclusion
   - 공유 자원을 사용할 경우 자원의 접근을 락을 이용해 통제한다.
2. Atomic operations
   - 원자적 연산을 이용한다(CAS)
3. Thread-local storage
   - 공유 자원의 사용을 최대한 줄여 각각의 스레드에서만 접근 가능한 저장소들을 사용함으로써 동시 접근을 막는다.
4. Immutable objects(불변 객체)
   - 공유 자원이 불변 객체를 사용하여 동시에 접근해도 스레드 안전성이 보장된다.
## Peterson's Algorithm 이 무엇이며, 한계점에 대해 설명해 주세요.
프로세스 간의 상호 배제를 보장하는 동기화 알고리즘이다.
- flag 배열 : 각 프로세스가 자신이 임계 구역에 들어가고자 하는 지 표시한다. `flag[i] = true` 프로세스 i 가 임계 구역에 들어가고자 한다는 것을 의미한다.
- turn 변수 : 두 프로세스 중 어느 쪽에 우선권이 있는지 나타낸다. 현재 자원을 사용할 차례를 지정하는 데 사용된다.

**한계점**
1. Busy waiting 방식을 사용하기 때문에 CPU 자원이 낭비된다.
2. 두 프로세스만 동기화할 수 있다. (3개 이상의 프로세스로 확장하기 어려움)

## Race Condition 이 무엇인가요?
여러 스레드나 프로세스가 동시에 접근하여 공유 자원의 값을 변경하려고 할 때 발생하는 문제
실행 순서에 따라 결과가 달라질 수 있는 상태
## Thread Safe를 구현하기 위해 반드시 락을 사용해야 할까요? 그렇지 않다면, 어떤 다른 방법이 있을까요?
1. CAS
2. Immutable Objects
3. Thread-safe 한 자료구조
   - 자바 Concourrent 자료구조
4. Thread-local storage
# 13. Thread Pool, Monitor, Fork-Join에 대해 설명해 주세요.

**Thread Pool**
Thread Pool 을 정리하기 전에 간단하게 Program, Process, Thread 에 대해 정리하고 가자

Program : 어떤 목적을 달성하기 위해 컴퓨터의 동작들을 하나로 모아 놓은 것
Process : 컴퓨터가 현재 실행중인 프로그램
Thread : Process 내에서 실행되는 여러 흐름의 단위

**Thread 만 사용해서 동시에 여러 작업을 할 수 있는 프로그램을 만들 수 있을 것인가?**

**Thread 를 단순하게 사용할 때**

<img width="1034" alt="스크린샷 2024-05-03 오후 1 35 10" src="https://github.com/user-attachments/assets/c1686804-c958-4637-9f6b-e373b85f3ed1" />

문제 1 
Thread 생성비용이 크기 때문에 요청에 대한 응답시간이 늘어난다.
- Java 는 One-to-One Threading-Model 로 Thread 를 생성하기 때문에 Thread 생성 비용이 많이 든다. 
문제 2
메모리 문제가 발생할 수 있고, CPU 오버헤드가 증가한다.
1. Process 의 처리 속도보다 빠르게 요청이 들어온다.
2. 새로운 Thread 가 계속 생성된다.
3. Thread 가 많아 질수록 메모리를 차지하고 Context-Switching 이 더 자주 발생한다.

이러한 문제점을 해결해서 동시요청을 처리 할 수 있는 방법
->  Thread Pool : Thread 를 허용된 개수 안에서 사용하도록 제한하는 시스템

**Thread Pool**

<img width="1094" alt="스크린샷 2024-05-03 오후 2 01 28" src="https://github.com/user-attachments/assets/f3ce459d-806e-424a-afc5-33d62d904562" />

문제 1 
미리 만들어 놓은 Thread 를 재사용할 수 있기 때문에 새로운 Thread 를 생성하는 비용을 줄일 수 있다.

문제 2
사용할 Thread 개수를 제한하기 때문에 무제한적으로 Thread 가 생성되지 않아서 방지 가능하다.

**Monitor**
상호 배제, 한정 대기, 진행의 융통성의 조건을 만족시키며 경쟁상태를 해결하는 방법 중 하나이다.

- **상호 배제** : 한 프로세스가 임계 영역에 들어갔을 때 다른 프로세스는 들어갈 수 없음
- **한정 대기** : 특정 프로세스가 임계 영역에 진입을 요청한 후 해당 요청이 승인되기 전까지 다른 프로세스가 임계영역에 진입하는 횟수를 제한하는 것을 말하며 이를 통해 특정 프로세스가 영원히 임계 영역에 들어가지 못하게 하는 것을 방지함
- **진행의 융통성** : 만약 어떠한 프로세스도 임계영역을 사용하지 않는다면 임계영역 외부의 어떠한 프로세스도 들어갈 수 있으며 이 때 프로세스끼리 서로 방해하지 않는 것을 말함

**Monitor** 는 둘 이상의 스레드나 프로세스가 공유 자원에 안전하게 접근할 수 있도록 공유자원을 숨기고 해당 접근에 대한 인터페이스만 제공하는 객체이다.
-> 공유자원에 대한 작업들을 순차적으로 처리한다.

<img width="445" alt="스크린샷 2024-05-03 오후 2 34 34" src="https://github.com/user-attachments/assets/47787866-0bbb-40b9-b639-c310e1681dbe" />

**Fork-Join**
큰 작업을 여러 개의 작은 작업으로 나누어 Thread 가 처리한 후 결과를 합치는 작업이다.
여러 작은 작업을 스레드가 처리할 수 있도록 한다.

<img width="576" alt="스크린샷 2024-05-03 오후 2 56 24" src="https://github.com/user-attachments/assets/146d5bef-676a-4734-a29b-5712a1780815" />

Java 에서의 Fork Join 은 Quicsort, Mergesort 와 같은 재귀 분할-정복 알고리즘과 함께 사용되도록 설계되어있다.
분할 단계 동안 별도의 작업이 fork 되고 원래 문제의 작은 부분집합이 할당된다.


## Thread Pool을 사용한다고 가정하면, 어떤 기준으로 스레드의 수를 결정할 것인가요?
1. CPU 의 코어 개수 task 의 성향에 따라 설정
- CPU-bound task 성향의 task -> 코어 개수만큼 또는 그 보다 몇 개 더 많은 정도로 설정
- I/O-bount task 성향의 task -> 코어 개수만큼 훨씬 많은 Thread 사용 (경험적으로 찾아야함)
## 어떤 데이터를 정렬 하려고 합니다. 어떤 방식의 전략을 사용하는 것이 가장 안전하면서도 좋은 성능을 낼 수 있을까요?
1. 병합 정렬 (Merge Sort)
   - 안정적인 정렬 방법으로, 데이터의 분할과 병합을 통해 정렬합니다.
   * 시간 복잡도는 O(n log n)으로 매우 효율적이며, 대규모 데이터셋에서도 잘 작동합니다.
2. 퀵 정렬 (Quick Sort)
   * 평균적으로 매우 빠른 성능을 보이는 정렬 알고리즘입니다.
   * 안정성은 보장되지 않지만, 대부분의 상황에서 빠른 정렬을 제공합니다.
3. 힙 정렬 (Heap Sort)
   * 안정적이며, 최악의 경우에도 O(n log n)의 성능을 보장하는 정렬 방법입니다.
   * 힙 구조를 활용하여 데이터를 정렬합니다.
4. 삽입 정렬 (Insertion Sort)
   * 작은 크기의 데이터셋에서 효과적이며 안정적인 정렬 방법입니다.
   * 이미 정렬된 부분이 있다면 빠르게 수행됩니다.
5. TimSort
   * Python의 기본 정렬 알고리즘으로, 병합 정렬과 삽입 정렬을 결합한 것입니다.
   * 안정적이면서도 효율적인 정렬을 제공합니다.
6. 외부 정렬 (External Sorting)
   * 대용량 데이터를 안정적으로 정렬하기 위한 방법으로, 외부 기억장치를 활용합니다.
   * 병합 정렬과 같은 기법을 사용합니다.

> 운영체제의 관점에서는 시스템의 자원 사용 최적화와 병렬 처리 능력을 활용하는 것이 중요하다. 대규모 데이터 처리 시 외부 정렬을 고려하고, 멀티코어 시스템에서는 병렬 퀵 정렬이나 병렬 병합 정렬을 사용하는 것이 적합하다.

# 14. 캐시 메모리 및 메모리 계층성에 대해 설명해 주세요
**메모리 계층 구조(Memory Hierachy) 란?**

<img width="710" alt="스크린샷 2024-05-05 오후 3 19 40" src="https://github.com/user-attachments/assets/93178cc9-3f67-4888-9e85-ab2e090d0758" />

레지스터, 캐시, 주기억장치, 보조기억장치로 구성되어 있다.
- 레지스터 : CPU 내의 작은 메모리, 휘발성, 속도 가장 빠름, 기억 용량이 가장 적음
- 캐시 : CPU 내의 L1, L2 캐시를 지칭한다. 휘발성, 속도 빠름, 기억 용랑이 적음
- 주기억장치 : RAM 을 가리킨다. 휘발성, 속도 보통, 기억 용량이 보통
- 보조기억장치 : HDD, SSD 를 말하며 비휘발성, 속도 낮음, 기억 용량이 많음

**계층이 존재하는 이유**

**더 빠른 접근과 처리속도 증가**
보통 많이 쓰는 것을 다시 많이 쓴다. 그렇기 때문에 특정 데이터에 더 많이 접근하게 되는데 좀 더 작은 캐시 메모리에 해당 데이터가 있으면 더 빠르게 해당 데이터에 접근이 가능하다. -> 처리속도도 증가하게 된다.

**비용의 효율성**
계층이 있고 캐싱 때문에 비용을 좀 더 효율적으로 쓸 수 있다.

**자원의 효율적 사용**
자주 접근하는 데이터는 빠른 메모리에, 덜 접근하는 데이터는 느린 메모리에 저장하여 자원을 효율적으로 사용할 수 있다.

**캐시 메모리**
CPU 와 주 메모리 사이의 속도 차이를 줄이기 위해 사용된다. 캐시는 CPU 가 빠르게 접근할 수 있는 작은 용량의 고속 메모리이다.

**캐시 메모리의 유형**
- L1 캐시 : CPU 코어에 가장 가까운 캐시로, 가장 작지만 가장 빠르다. L2 일부를 저장
- L2 캐시 : L1 보다 조금 느리고 크키가 크며, 주로 L1 캐시의 데이터를 보완한다. L3 일부를 저장
- L3 캐시 : CPU 내의 여러 코어가 공유하는 캐시로, L1 과 L2 보다 느리지만 더 큰 용량을 가지고 있다. 메인메모리 일부 내용을 저장

CPU 상위 계층부터 자료를 찾아 없으면 차례대로 하위계층으로 내려가면서 찾음
L1 -> L2 -> L3 -> 메인 메모리 -> 하드디스크

## 캐시 메모리는 어디에 위치해 있나요?
CPU 에 아주 가까운 곳에 위치하거나 아예 내장되어 있다.  

## L1, L2 캐시에 대해 설명해 주세요.

**L1 캐시**
가장 먼저 참조되며 가장 빠른 캐시이다. 명령 캐시와 데이터 캐시 분리되어 있다.
**L2 캐시**
프로세서 칩 내부의 다른 공유 장치들과의 버스 인터페이스 속도를 높이기 위해 사용된다.

## 캐시에 올라오는 데이터는 어떻게 관리되나요?
**캐시 등록**
인덱스
- 캐시에서 몇 번째 캐시 라인인지 구분하는 색인 번호
캐시 라인 등록 정보
- 메인메모리에 있는 하나의 데이터 블록이 하나의 캐시 라인으로 복사되며 해당 캐시라인에 **등록정보** 생성
- 현재 저장된 데이터 블록이 유효한지 표시하는 플래그 비트를 포함

**캐시 쓰기 정책**
캐시의 블록이 변경되었을 때 메인메모리의 블록을 갱신하는 방법과 시기를 정하는 것

**연속 기록(write-through) 캐시**
- CPU 가 캐시와 메인메모리 두 군데 데이터를 **정상적으로 함께 갱신**하는 방식
- 속도가 빠른 캐시가 먼저 업데이트된 후 느린 메인 메모리에 기록이 완료된다.
  - 구조는 간단하지만 캐시가 갱신될 때, 느린 메인메모리를 매번 함께 액세스하므로 성능 저하

**후기록(write-back) 캐시**
- CPU 가 캐시의 데이만 우선 변경하고 **메인메모리는 나중에 갱신**하는 방식
  - 캐시의 자료가 교체될 때 메인메모리에 없는 내용이면 메인메모리로 복사한 후 캐시를 비움

<img width="793" alt="스크린샷 2024-05-05 오후 4 17 36" src="https://github.com/user-attachments/assets/f71540ac-0190-4d2b-bd7c-9dc35b7decb0" />

**캐시 교체 정책**
- 캐시의 내용 중 교체되어 나갈 자료를 결정하는 것
- 가장 이상적인 목표는 **미래에 가장 오랫동안 필요로 하지 않는 자료를 교체**하는 것

| 알고리즘                                     | 교체 정책                   |
|------------------------------------------|-------------------------|
| 선입 선출<br>(first-in first-out, FIFO)      | 가장 먼저 들어온 자료를 먼저 교체<br> |
| 최소 최근 사용<br>(least recently used, LRU)   | 최근에 가장 적게 사용된 자료를 먼저 교체 |
| 최대 최근 사용<br>(most recently used, MRU)    | 최근에 가장 많이 사용된 자료를 먼저 교채 |
| 최소 빈도 사용<br>(least frequently used, LFU) | 사용 횟수가 가장 적은 자료를 먼저 교체  |

## 캐시간의 동기화는 어떻게 이루어지나요?
여러 캐시 사이에서 데이터의 일관성을 유지하기 위해서는 캐시 간의 동기화가 필요하다.
캐시 일관성 프로토콜을 사용하여 관리된다. 캐시 일관성 프로토콜은 시스템의 다른 부분에 있는 캐시 사이의 데이터 복사본이 최신 상태로 유지되도록 보장한다.
**캐시 일관성 프로토콜의 유형**
1. MSI 프로토콜
   - Modified : 데이터가 수정되어 원본과 다르며, 이 상태의 데이터는 해당 캐시에만 존재해야 한다.
   - Shared : 데이터가 여러 캐시에 저장되어 있으면 모든 복사본이 일치한다.
   - Invalid : 데이터가 더 이상 유효하지 않거나 최신 상태가 아니다.
2. MESI 프로토콜
   - Exclusive : 데이터가 한 캐시에만 있고 수정되지 않은 상태이다.
3. MOESI 프로토콜
   - Owner : 데이터가 수정될 수 있으며, 다른 캐시에 데이터를 제공할 책임이 있다.

**캐시 동기화 기법**
1. **버스 스누핑** :모든 캐시가 메모리 버스를 감시(스누핑)하여 다른 캐시의 읽기/쓰기 작업을 확인하고, 필요에 따라 자신의 캐시 상태를 조정한다.
2. **디렉토리 기반 동기화** : 중앙 디렉토리가 모든 캐시 라인의 상태를 추적하며, 데이터 접근 시 디렉토리를 참조하여 적절한 조치를 취한다.

## 캐시 메모리의 Mapping 방식에 대해 설명해 주세요.

**캐시의 매핑 방식**
- 메인메모리에 있는 다수의 데이터 블록들이 소수의 캐시 라인을 공유하는 방법
  - 메인메모리와 캐시의 자료를 대응시키는 방법
- CPU 가 발생시킨 메인메모리 주소와 캐시 라인 자료를 1:1로 매핑
  - 완전 연관(fully associative) 캐시 : 메인메모리의 데이터 블록이 **아무 캐시 라인**에나 들어감
  - 직접 매핑(direct-mapped) 캐시 : 데이터 블록이 **지정된 캐시 라인**에만 들어감
  - 세트 연관(set-associative) 캐시 : 데이터 블록이 복수의 캐시 라인을 묶은 **지정된 세트**에만 들어감

<img width="967" alt="스크린샷 2024-05-05 오후 4 37 17" src="https://github.com/user-attachments/assets/123c9edf-d132-4a1b-9272-85038b01df88" />

## 캐시의 지역성에 대해 설명해 주세요.
**시간적 지역성(tempral locality)**
- CPU 에서 한번 참조한 프로그램이나 데이터는 조만간 다시 참조될 가능성이 높음
 -> 짧은 시간 내 특정 데이터나 자원을 재사용하는 현상
**공간적 지역성(spatial locality)**
- 한번 참조된 데이터 주변에 인접한 데이터는 같이 참조될 가능성이 높음
-> 인접 저장된 배열 데이터
- 대부분의 프로그램 명령어는 분기가 발생하기 전까지 기억장치에 저장된 순서대로 실행

## 캐시의 지역성을 기반으로, 이차원 배열을 가로/세로로 탐색했을 때의 성능 차이에 대해 설명해 주세요.

리눅스는 페이징으로 메모리를 관리하며, 기본적으로 페이지의 크기는 4KB 이다.

<img width="682" alt="스크린샷 2024-05-05 오후 5 47 44" src="https://github.com/user-attachments/assets/c37879d7-c287-4142-87d9-278db880fa84" />

처음 4 를 방문했을 때에는 캐시에 저장되어 있지 않아 캐시 미스가 나지만 그 이후로 6,8,9 를 방문했을 때에는 모든 값들이 캐시에 저장되기 때문에 캐시 히트가 발생한다.(공간적 지역성)

그렇기 때문에 가로로 배열을 순회하면 공간적 지역성 때문에 계속 캐시 히트가 발생하지만, 세로로 배열을 하면 그렇지 않아 성능 차이가 많이 난다.
## 캐시의 공간 지역성은 어떻게 구현될 수 있을까요? (힌트: 캐시는 어떤 단위로 저장되고 관리될까요?)

원하는 데이터를 메모리에서 찾아서 캐시에 넣어줄 때 그 데이터 하나만 넣지 않는다. 캐시 메모리는 캐시 라인이라는 단위로 저장한다. 보통 32바이트, 64바이트 그 이상일 수 있다. 
이를 통해 공간 지역성을 구현할 수 있다.

# 15. 메모리의 연속할당 방식 세 가지를 설명해주세요.(first-fit, best-fit, worst-fit)

**연속할당 방식**
프로세스를 메모리에 올릴 때 주소 공간을 **물리적 메모리 한 곳에 연속적으로 쌓는** 방식이다.

연속할당 방식에는 **고정분할 방식**, **가변분할 방식**이 있다.

**고정분할 방식**
물리적 메모리를 주어진 개수만큼의 **영구적인 분할로 미리 나누고 각 공간에 하나의 프로세스를 적재**하는 방법
(각 나눈 공간은 모두 동일하게, 다르게 할 수 있다.)

**문제점**
외부 단편화, 내부 단편화가 발생할 수 있다.

![image](https://github.com/user-attachments/assets/420786aa-0dd3-4f75-a037-ac304de54a61)

![image 2](https://github.com/user-attachments/assets/98bbc396-4665-48d4-82d1-950d07bb52ce)

### 가변분할 방식
메모리에 적재되는 **프로그램의 크기에 따라 분할의 크기, 개수가 동적으로 변하는** 방법

<img width="908" alt="스크린샷 2025-03-11 오전 1 52 49" src="https://github.com/user-attachments/assets/6d3c84cd-7d86-49f1-8b89-1f6f9d06811d" />

이미지를 설명하면, 프로그램들의 크기대로 메모리 공간의 P1, P2, P3, P4, P5를 적재 시켰다. P2, P4가 종료돼서 메모리 공간이 비게 된다.

-> 프로세스가 종료되면 메모리 내의 여러 곳의 **메모리의 빈공간**이 생긴다.
새로운 프로그램을 메모리의 빈공간 중 어디에 올릴 것인지 결정하는 문제가 생긴다. 이러한 문제를 동적 메모리 할당 문제라고 부른다.

동적 메모리 할당 문제를 해결하는 대표적인 방법으로는 **first-fit**, **best-fit**, **worst-fit** 이 있다.

### first-fit(최초 적합)

크기가 N 이상인 빈공간 중 가장 먼저 찾아지는 곳에 프로그램을 적재하는 방법이다.

<img width="708" alt="스크린샷 2025-03-11 오전 1 53 08" src="https://github.com/user-attachments/assets/4461afa3-12e2-4655-a682-313b228d3436" />

메모리의 빈 공간을 차례대로 체크를 하면서 빈 공간이 프로그램 크기보다 작으면 패스하고 프로그램보다 크거나 같으면 그 공간에 프로그램을 적재한다.
-> 그 뒤에 나오는 빈 공간을 체크 할 필요가 없으므로 **시간적인 측면**에서 효율적이다

### best-fit(최적 적합)
크기가 N 이상인 가장 작은 빈 공간을 찾아 그곳에 프로그램을 할당하는 방법

<img width="884" alt="스크린샷 2025-03-11 오전 1 53 22" src="https://github.com/user-attachments/assets/57808d04-22c3-42ba-8f29-ede648b3e746" />

메모리의 모든 빈 공간을 탐색을 해서 새로운 프로그램을 넣을 수 있는 공간 중에서 가장 작은 공간에 적재한다.
빈 공간 A,B,C 중에서 P6 은 B,C 에 들어갈 수 있지만 C 가 더 작아 C 에 적재한다.

-> 모든 빈 공간을 탐색해야 하니 시간적인 오버헤드가 발생할 수 있다. 하지만 가장 최적화된 공간에다가 할당을 하기 때문에 **공간적인 측면**에서 효율적이다.
### worst-fit
빈 공간 중 **가장 크기가 큰 곳**에 새로운 프로그램을 할당하는 방법
이 방법도 best-fit 가 마찬가지로 모든 공간을 탐색해야 하므로 시간적인 오버헤드가 발생한다.

<img width="892" alt="스크린샷 2025-03-11 오전 1 53 33" src="https://github.com/user-attachments/assets/8c2326c4-56dc-4541-b028-6ea3077f1c24" />

가용 공간 중 가장 크기가 큰 B 에 적재한다.
## worst-fit 은 언제 사용할 수 있을까요
worst-fit 은 가장 큰 빈 공간에 할당함으로써, 나중에 크키가 큰 프로세스의 요청이 있을 때 대응할 수 있는 여유 공간을 많이 확보할 수 있다. 

1. 큰 메모리 요구가 예상될 때 
2. 단편화 최소화를 목표로 할 때 
## 성능이 가장 좋은 알고리즘은 무엇일까요?
first-fit 이 일반적으로 가장 빠른 할당 속도를 제공하고 사용률은 best-fit 이 가장 좋다.


# 16. Thrashing 이란 무엇인가요?
프로세스에 충분한 프레임이 없는 경우, 즉 프로세스가 작업 집합에 필요한 최소 프레임 수를 확보하지 못한 경우 프로세스는 빠르게 페이지 폴트를 일으킨다. 이 시점에서 일부 페이지를 교체해야 한다. 하지만 모든 프로세스가 활발히 사용 중이기 때문에 곧바로 다시 필요할 페이지를 교체해야만 한다. 결과적으로 반복해서 페이지 폴트가 일어난다. 이처럼 과도한 페이징 작업을 **스래싱(Thrashing)**이라고 한다.

어떤 프로세스가 실제 실행보다 더 많은 시간을 페이징에 사용하고 있으면 스래싱이 발생했다고 한다.
심각한 성능 문제를 야기한다.
## Thrashing 발생 시, 어떻게 완화할 수 있을까요?
1. 작업 집합 모델
   - 지역성 이론을 바탕으로 최근 일정 시간 동안 참조된 페이지들을 집합으로 만들고, 이 집합에 있는 페이지들을 물리 메모리에 유지한다.
2. 페이지 폴트 빈도
   - 페이지 폴트율이 너무 높으면 그 프로세스가 더 많은 프레임이 필요하다는 의미이고, 너무 낮으면 많은 프레임을 갖고 있다는 것을 의미
   - 페이지 폴트율의 상한,하한을 정하고 그에 따라 프레임 수를 조절한다.
3. 충분한 물리 메모리를 사용한다.

 # 17. 가상 메모리란 무엇인가요?
실제 물리 메모리 크기보다 큰 메모리 공간을 프로그램에 제공하는 기술이다. 
이를 통해 프로그램은 사용 가능한 실제 메모리보다 더 많은 메모리를 사용하는 것처럼 동작할 수 있다.

## 가상 메모리가 가능한 이유가 무엇일까요?
실제 프로그램들을 살펴보면 많은 경우에 프로그램 전체가 한꺼번에 실제 메모리에 올라와 있어야 하는 것은 아니다.
- 프로그램에는 잘 발생하지 않는 오류 상황을 처리하는 코드가 존재한다. 이러한 오류들은 실질적으로 거의 발생하지 않아 거의 실행되지 않는다.
- 배열, 리스트, 테이블 등은 필요 이상으로 많은 공간을 점유하는 수가 있다. 배열이 실제로는 10 x 10 정도만 사용되는데 100 x 100 으로 선언될 수 있다.
## Page Fault가 발생했을 때, 어떻게 처리하는지 설명해 주세요.
**Page Fault**
유효 비트가 0인 공간(=페이지가 메모리에 적재되지 않았다)에 접근하려고 할 때 발생하는 인터럽트이다.

![img1 daumcdn](https://github.com/user-attachments/assets/2836c61f-82da-41d8-bad9-b8e8450f5de6)

1. 페이지 테이블을 통해 필요한 페이지가 유효한지 검사한다.
2. 만약 invalid한 페이지에 대한 참조라면 그 프로세스는 중단된다. 만약 valid한 페이지가 아직 메모리에 올라오지 않았다면, 그것을 보조저장장치로부터 가져와야한다.
3. 빈 공간, 즉 가용 프레임을 찾는다
4. 보조저장장치에 새로 할당된 프레임으로 해당 페이지를 읽어 들이도록 요청한다.
5. 보조저장장치에 읽기가 끝나면, 이 페이지가 이제는 메모리에 있다는 것을 알리기 위해 페이지 테이블을 갱신하며, 프로세스가 유지하고 있는 내부 테이블을 수정한다.
6. 트랩에 의해 중단되었던 명령어를 다시 수행한다. 
## 페이지 크기에 대한 Trade-Off를 설명해 주세요.
페이지 크기가 크다면?
- 하나의 페이지에 더 많은 주소를 담을 수 있어 페이지 테이블 크기가 줄어들어 페이지 테이블 관리가 더 쉬워진다.
- 내부 단편화 증가
- 페이지 폴드 빈도 증가
페이지 크키가 작다면?
- 주소 공간을 여러 페이지로 나누기 때문에 페이지 테이블이 커져 페이지 테이블 관리 비용이 증가한다.
- 내부 단편화 감소
- 페이지 폴트 빈도 감소
## 페이지 크기가 커지면, 페이지 폴트가 더 많이 발생한다고 할 수 있나요?
페이지 크키와 페이지 폴트의 관계가 예시를 어떻게 두냐에 따라서 달라지는 것 같습니다.

200kb 프로세스에 페이지 크기가 200kb이면 딱 한 번의 페이지 폴트가 발생하고 그 이후에는 발생하지 않고, 만약 페이지 크기가 1바이트라고 하면 페이지 폴트가 102400번 발생


페이지 크기가 커지면 -> 각 페이지에 불필요한 데이터들이 올라가게 되어 페이지 폴드가 증가가 증가한다.

> 만약 연속된 메모리에 접근을 한다고 하면 페이지 크기가 커기면 페이지 폴트가 오히려 줄어들 수 있지 않을까?

## 세그멘테이션 방식을 사용하고 있다면, 가상 메모리를 사용할 수 없을까요?

**세그멘테이션**
프로세스의 메모리 부분을 가변적으로 여러 개로 나뉘어 물리 메모리에 비연속적으로 할당하는 방식이다.

<img width="769" alt="스크린샷 2024-11-11 오후 2 41 48" src="https://github.com/user-attachments/assets/5cb7e09c-b644-40e6-b272-83478ea7acfc" />

세그먼테이션도 페이징처럼 매핑 테이블을 사용한다. 세그먼테이션 크기를 나타내는 limit, 물리 메모리 상의 시작 주소를 나타내는 address가 있다. 페이징은 같은 크기의 페이지로 분할 하기 때문에 매핑 테이블에 크기 정보를 유지할 필요가 없지만 세그먼테이션을 프로세스의 크기에 따라 메모리를 분할하기 때문에 매핑 테이블에 크기 정보를 포함한다.

# 18. 세그멘테이션과 페이징의 차이점은 무엇인가요?
**페이징**
를 수 있다. CPU가 바라보는 주소공간인 논리 주소공간을 페이지라는 일정 단위로 자르고, 실제 메모리 주소공간인 물리 주소공간은 프레임이라는 페이지와 동일한 단위로 자른 뒤, 페이지를 프레임에 할당하는 가상메모리 관리 기법이다

**세그멘테이션**
메모리를 논리적 단위인 '세그먼트'로 나누어 관리하는 방식이다. 각 세그먼트는 코드, 데이터, 스택 등 기능적으로 구분되며 크기가 서로 다

**차이점**
- 메모리를 분할하는 기준이 다르다.
  - 페이징 : 일정한 단위
  - 세그멘테이션 : 논리적 단위
- 단편화 발생의 차이
  - 페이징 : 내부 단편화가 발생할 수 있다.
  - 세그멘테이션 : 외부 단편화가 발생할 수 있다.

## 페이지와 프레임의 차이에 대해 설명해 주세요.
**페이지**
가상 메모리에서 사용되는 일정한 크기의 메모리 블록이다.
**프레임**
실제 물리 메모리에서 사용되는 일정한 크기의 메모리 블록이다.


## 내부 단편화와, 외부 단편화에 대해 설명해 주세요.

![image 7](https://github.com/user-attachments/assets/0e0d484e-0159-4e6e-b4ca-f8f0424a9b48)

**외부 단편화** 
할당된 메모리 블록 사이에 사용하지 못하는 작은 빈 공간들이 발생하는 현상을 의미합니다

![image 8](https://github.com/user-attachments/assets/87e93e38-ead7-4fe2-926e-d10cc0b06527)

**내부 단편화** 
메모리 할당 시 프로세스가 실제로 요청한 메모리보다 더 큰 메모리 블록이 할당되어, 그 블록 안에서 사용되지 않는 부분이 발생하는 현상을 의미합니다.
## 페이지에서 실제 주소를 어떻게 가져올 수 있는지 설명해 주세요.

<img width="1559" alt="스크린샷 2024-11-11 오후 3 03 09" src="https://github.com/user-attachments/assets/c4591348-1c78-47e8-bcee-75ad9d027771" />

페이지 테이블을 활용해서 실제 주소를 가져올 수 있다.
페이지 번호와 프레임 번호를 이어주는 역할을 한다.
<페이지 번호, 오프셋> 으로 이루어진 논리 주소는 페이지 테이블을 통해 <프레임 번호, 변위> 로 변환한다. 
1. 페이지 테이블 참조
2. 프레임 주소로 변환
3. 오프셋을 더해서 접근할 데이터의 물리적 메모리 주소 확보
## 어떤 주소공간이 있을 때, 이 공간이 수정 가능한지 확인할 수 있는 방법이 있나요?

<img width="618" alt="스크린샷 2024-11-11 오후 3 07 26" src="https://github.com/user-attachments/assets/f99e9085-f4f2-4273-beb5-02e6a35acc27" />

- access bit : 페이지에 접근이 이루어졌는지를 나타내는 비트
- modified(dirty) bit : 페이지가 수정되었는지를 나타내는 비트
- valid bit : 페이지가 유효한 메모리 영역에 할당되었는지 여부를 나타내는 비트
- read bit: 해당 페이지가 읽기 권환이 있는지를 나타내는 비트
- write bit : 해당 페이지가 쓰기 권환이 있는지를 나타내는 비트
- excute bit : 해당 페이지가 실행 권환이 있는지를 나타내는 비트이다.

write bit을 통해 쓰기 권환이 있는지를 확인하여 수정 가능한지 알 수 있다.
## 32비트에서, 페이지의 크기가 1kb 이라면 페이지 테이블의 최대 크기는 몇 개일까요?
32 비트 -> 2^32 개의 주소 공간
페이지 크기 1KB -> 각 페이지가 2^10개의 주소 공간
페이지 수 = 전체 주소 공간 / 페이지 크기 = 2^32 / 2^10 = 2^22 이다.

페이지 테이블 크기 = 페이지 수 * 페이지 테이블 항목 크기(4바이트)
따라서 페이지 테이블 크기는 = 2^22 * 2^2바이트 = 2^24 -> 16MB

## 32비트 운영체제는 램을 최대 4G 까지 사용할 수 있습니다. 이 이유를 페이징과 연관 지어서 설명해 주세요.
32비트 운영체제는 최대 2^32 개의 주소 공간을 가질 수 있다. 즉 4GB이다.
프로세스는 32비트 가장 주소 공간를 사용할 수 있다. 따라서 가상 주소에서 물리 주소를 매핑할 때 32비트 이상의 가상 주소로 매핑할 수 없다.

만약 32비트 운영체제에서 램을 5GB를 사용한다고 해도 4G 이상의 물리 주소는 가상 주소로 매핑할 수가 없다.
따라서 실제 물리 메모리 공간이 존재한다고 하더라도 그 공간은 접근을 할 수 없다.
## C/C++ 개발을 하게 되면 Segmentation Fault 라는 에러를 접할 수 있을텐데, 이 에러는 세그멘테이션/페이징과 어떤 관계가 있을까요?
**Segmentation Fault**
프로그램이 허용되지 않은 메모리 영역에 접근을 시도하거나, 허용되지 않은 방법으로 메모리 영역에 접근을 시도할 경우 발생한다.

**1** **메모리 보호와 접근 제어**:
- 운영체제는 각 프로세스에 할당된 메모리 구역을 세그먼트(segment)와 페이지(page) 단위로 관리합니다. 세그먼트는 주로 **코드, 데이터, 스택**과 같은 논리적 메모리 영역을 의미하며, 페이지는 일정 크기로 나누어진 메모리 블록을 뜻합니다.
* 각 세그먼트와 페이지는 **접근 권한**이 설정되어 있어서, 프로그램이 허용되지 않은 메모리 구역을 읽거나 쓸 때, 운영체제는 이를 감지하여 **Segmentation Fault**를 발생시킵니다.
**2** **잘못된 메모리 접근**:
- 프로세스가 자신에게 할당되지 않은 세그먼트나 페이지에 접근할 경우, 예를 들어 NULL 포인터를 역참조하거나 할당하지 않은 메모리 공간에 접근하려고 하면, 운영체제는 해당 접근을 차단하고 Segmentation Fault를 발생시킵니다.
- 이는 페이징과 세그멘테이션이 **각 프로세스가 독립적인 메모리 공간을 사용하도록 보호**하기 때문에 발생하는 현상입니다.
**3** **페이지 폴트와의 관계**:
- Segmentation Fault는 **페이지 폴트(page fault)**와도 관련이 있습니다. 페이지 폴트는 프로그램이 **아직 메모리에 로드되지 않은 페이지**에 접근할 때 발생하지만, 운영체제는 이 경우 필요한 페이지를 메모리에 로드한 후 다시 접근을 시도하게 합니다.
* 그러나, 접근하려는 페이지가 프로세스의 메모리 영역에 없는 경우, 페이지 폴트는 Segmentation Fault로 이어집니다. 이처럼 Segmentation Fault는 페이지 폴트 중에서도 **접근 권한이 없거나, 프로세스에 할당되지 않은 메모리 공간에 접근**할 때 발생합니다.

- NULL 포인터 역참조
- 범위를 벗어난 배열 접근
- 해체된 메모리에 접근 

# 19. TLB는 무엇인가요?
메인 메모리에 페이지 테이블을 저장하면 컨텍스트 스위칭은 빨라지지만 메모리 액세스 시간이 느려질 수 있다.
- 페이지 번호를 기준으로 PTBR 오프셋의 값을 사용하여 페이지 테이블 항목을 찾는다. (메모리 접근)
- 페이지 테이블에서 얻은 프레임 번호, 페이지 오프셋을 결합하여 실제 메모리 주소에 액세스 한다. (메모리 접근)
-> 총 2번 메모리 액세스가 필요하다. 이 문제를 해결하기 위해 TLB가 사용된다.

**TLB**
자주 참조되는 페이지 테이블 정보를 저장하는 하드웨어 캐시이다.

![img1 daumcdn 2](https://github.com/user-attachments/assets/9d1c6874-8027-4cb8-9721-22f0a3e6585a)

## TLB를 쓰면 왜 빨라지나요?
페이지 테이블은 메인메모리에 있다. 그렇기 때문에 CPU가 메인 메모리에 2번 접근해야 원하는 데이터를 얻을 수 있다.
- 페이지 테이블 접근
- 페이지 테이블 정보를 바탕으로 실제 메모리 접근

TLB를 사용하면 페이지 테이블에 접근하는 단계를 줄일 수 있다. 
MMU 내부에서 바로 실제 메모리 주소를 얻을 수 있다. 
## MMU가 무엇인가요?
- 논리 주소를 물리 주소로 변환해준다.
- 메모리 보호나 캐시 관리 등 CPU가 메모리에 접근하는 것을 총 관리해주는 하드웨어
MMU가 지원되지 않으면, 물리 주소를 직접 접근해야 하기 때문에 부담이 있다.
MMU는 사용자가 기억장소를 일일이 할당해야 하는 불편을 없애준다.
프로세스의 크기가 실제 메모리의 용량을 초과해도 실행될 수 있게 해준다.

## TLB와 MMU는 어디에 위치해 있나요?

<img width="803" alt="스크린샷 2024-11-12 오후 12 37 04" src="https://github.com/user-attachments/assets/3ff38fec-13dd-48ba-8c51-e1210e60e46f" />

TLB와 MMU 모두 CPU 내부에 위치한다.
## 코어가 여러개라면, TLB는 어떻게 동기화 할 수 있을까요?
코어가 여러개라면 각 코어마다 TLB를 가지게 된다.
-> 서로 다른 TLB가 같은 주소에 대한 entry를 가질 수 있음을 시사한다.
이렇게 되면 페이지 테이블 엔트리가 변경될 때마다 해당 주소에 대한 엔트리를 갖고 있는 모든 TLB가 해당 엔트리를 버려야 한다. 

이때 해당 TLB를 소유하고 있는 코어만이 해당 TLB를 수정할 수 있고, 또한 OS는 TLB가 어떤 page를 가지고 있는지 모르기 때문에 OS는 모든 프로세서에게 엔트리를 지우라는 인터럽트를 날려야만 한다.

그리고 모든 프로세서가 무사히 요청을 완료했을 때 비로소 동작이 끝이 난다. 
이 동작을 **TLB shootdown**이라고 한다.

![shoot](https://github.com/user-attachments/assets/ca9d474a-0c3f-49d0-98fe-3201ea257f81)

## TLB 관점에서, Context Switching 발생 시 어떤 변화가 발생하는지 설명해 주세요.

프로세스 A의 VPN(Virtual Page Number) 1에 대한 주소 변환 정보가 TLB에 저장되어 있는데 컨텍스트 스위칭이 일어나 프로세스 B가 실행되었다고 가정하자.
프로세스 B도 VPN 1을 사용하려고 할 때 TLB에 동일한 VPN 1이 있다는 이유만으로 주소 변환을 수행하면 잘못된 물리적 주소로 접근할 수 있다. 이렇게 되면 각 프로세스가 자신이 아닌 다른 프로세스의 메모리 공간에 접근하게 되는 문제가 발생한다.

결국, Context switch가 일어나게 되면 이전 프로세스의 정보를 담고 있는 TLB는 더 이상 유효하지 않다.
TLB consistency를 유지하기 위해서 가장 간단한 방법은 TLB를 flush 하는 것이다. 하지만 너무 많은 overhead가 발생한다.

TLB 엔트리를 재사용하는 방법 **tagged TLB** 가 좀 더 효율적이다.

Tagged TLB의 핵심은 해당 translation을 소유했던 프로세스의 id를 저장하는 것이다.
VA 뿐만 아니라 PID도 조사하여 TLB hit를 판단하는 것이다.

컨텍스트 스위치가 일어나더라도 아무것도 바꿀 필요가 없기 때문에 추가 동작도 없고 TLB 엔트리도 재사용할 수 있다는 장점이 있다.

![tagged](https://github.com/user-attachments/assets/4137921f-2ebb-4980-831c-6f3e5757f2ca)

# 20. 동기화를 구현하기 위한 하드웨어적인 해결 방법에 대해 설명해 주세요.
**메모리 장벽(Memory Barriers)**
- 메모리의 모든 변경 사항을 다른 모든 프로세서로 전파하는 명령어를 제공하여 다른 프로세서에서 실행 중인 스레드에 메모리 변경 사항이 보이는 것을 보장한다. 이러한 명령어를 **메모리 장벽**이라고 한다.
- CPU가 특정 명령어가 실행되기 전 또는 후에 메모리 접근 순서를 보장하도록 강제하는 명령어입니다.
-> CPU 내부의 명령어 재정렬로 인한 데이터 불일치를 방지할 수 있고, 다중 프로세서 시스템에서 순서가 보장된 메모리 접근을 위해 사용된다.

**test_and_set()**
```c
boolean test_and_set(boolean *target){
	boolean rv = *target;
	*target = true;
	
	return rv;
}
```
- 이 명령어는 원자적으로 실행 된다.
- 만약 두 코어에서 이 명령어가 동시에 실행된다면 어떤 임의의 순서로 순차적으로 실행된다.
- 만약 기계가 test_and_set()을 지원한다면 false로 초기화되는 lock이라는 boolean 변수를 선언하여 상호 배제를 구현할 수 있다.

- Busy-waiting 으로 인한 CPU 낭비

**compare_and_swap()**
```c
int compare_and_swap(int *value, int expected, int new_value) {
	int temp = *value;
	if (*value == expected) {
		*value = new_value;
	}
	return temp;
}
```
- value의 값이 expected와 같으면 value를 new_value로 변경하고 기존의 value 값을 반환하는 함수이다.
- ABA 문제 : CAS는 메모리 위치의 값이 예상한 값과 일치할 때만 값을 변경한다. 하지만 중간에 값이 A -> B -> A 로 변경될 수 있는 상황에서는 CAS가 성공해도 데이터가 일관되지 않을 수 있다.

## volatile 키워드는 어떤 의미가 있나요?

<img width="862" alt="스크린샷 2024-10-15 오후 2 53 01" src="https://github.com/user-attachments/assets/c0ca45af-9e47-4ed8-9a23-d2fcc706e45a" />

- 각 스레드가 runFlag 값을 사용하면 CPU는 runFlag를 캐시 메모리에 불러온다.
- 그 이후에는 캐시 메모리에 있는 runFlag를 사용한다.

**문제상황**
main 스레드에서 runFlag 값을 false로 변경하게 된다면 main 스레드가 사용하는 캐시 메모리에 있는 runFlag만 false로 변경하게 된다. 메인 메모리에 있는 runFlag값이 언제 반영되는지는 CPU 설계방식과 종류의 따라 다르다.

이렇게 멀티 코어에서 **캐시의 일관성**이 해치는 문제가 발생한다
- 데이터 일관성을 유지하기 위해 캐시 메모리에 성능을 포기하는 대신에 값을 읽고 쓸때 모두 메인 메모리에 직접 접근하는 방법이다.
- 캐시 메모리를 사용할 때 보다 성능이 느려지는 단점이 있기 때문에 필요한 곳에만 사용하는 것이 좋다.
## 싱글코어가 아니라 멀티코어라면, 어떻게 동기화가 이뤄질까요
싱글코어와 멀티코어에서의 동기화 차이가 캐시 일관성 프로토콜의 차이라고 보면 되나요?

# 21. 페이지 교체 알고리즘에 대해 설명해 주세요.

1. FIFO : 가장 오래된 페이지 교체
2. LRU : 가장 오랫동안 사용하지 않은 페이지 교체
3. LFU : 가장 적게 사용된 페이지 교체
4. Optimal : 앞으로 가장 오랫동안 사용되지 않을 페이지 교체(구현하기 힘듦, 어떤 페이지가 미래에 오랫동안 사용되지 않을지를 파악하기 어려움)
##  LRU 알고리즘은 어떤 특성을 이용한 알고리즘이라고 할 수 있을까요?
시간지역성의 특성을 이용한 알고리즘이다. 
> 시간지역성 : 최근에 사용한 데이터가 가까운 시간 내에 다시 사용될 가능성이 높다.

가장 오랫동안 사용하지 않은 페이지를 교체하여 메모리 내의 페이지 교체율을 낮출 수 있다.

## LRU 알고리즘을 구현한다면, 어떻게 구현할 수 있을까요?
프레임들을 최근 사용된 시간 순서로 파악할 수 있어야 한다.
1. **counters(계수기)**
- 각 페이지 항목마다 사용 시간 필드를 넣고 CPU에 논리적인 시계나 계수기를 추가한다.
- 메모리가 증가될 때마다 시간은 증가한다.
- 페이지가 참조될 때마다 페이지의 사용 시간 필드에 시간 레지스터의 내용이 복사된다. -> 페이지의 마지막 참조 시간을 유지할 수 있다.
- 시간 값이 가장 작은 페이지가 교체된다.
- 페이지 테이블이 변경될 때마다 시간 값을 관리해야 하며 시간 값의 오버플로우도 고려해야한다.

2. **양방향 링크드리스트**

<img width="207" alt="스크린샷 2024-11-14 오후 4 58 22" src="https://github.com/user-attachments/assets/97867498-39f8-45bc-8a7d-021a4c5e41f6" />

## LRU 알고리즘의 단점을 설명해 주세요. 이를 해결할 수 있는 대안에 대해서도 설명해 주세요.
페이징 시스템인 경우에 LRU, LFU 알고리즘 같은 경우 운영체제가 페이지의 참조 시간을 알 수 없다.
왜냐하면 페이지 폴트가 발생했을 때만 CPU의 제어권이 운영체제에게 넘어가기 때문이다.
페이지가 이미 메모리에 존재하는 경우 참조시각 등의 정보를 운영체제가 정확히 알 수 없다.

**Clock Algorithm**
LRU의 근사 알고리즘이다.
- Second Chance Algorithm, NUR, NRU 로 불린다.
**동작 방식**
- 페이지 폴트 발생 시 `Reference bit`을 사용해서 교체 대상 페이지 선정(circular list)
* `reference bit`가 0인 것을 찾을 때까지 포인터를 하나씩 앞으로 이동
* 포인터 이동하는 중에 `reference bit` 1은 모두 0으로 바꿈
* `Reference bit`이 0인 것을 찾으면 그 페이지를 교체
* 한바퀴 되돌아와서도(=second chance) 0이면 그때는 replace 교체당함
* 자주 사용되는 페이지라면 second chance가 올때 1(시계바늘이 돌때 한번 더 참조되었으니 최근에 참조되었다는 뜻)

**Clock Algorithm의 개선**
* reference bit과 modified bit(dirty bit)을 함께 사용
* reference bit = 1 : 최근에 참조 된 페이지
* modified bit = 1 : 최근에 변경된 페이지 (I/O, write를 동반하는 페이지 - 한번 쓰여졌기 때문에 backing store에 수정된 내용을 반영을 한다음에 메모리에서 내려야 함)

# 22. File Descriptor와, File System에 에 대해 설명해 주세요.
**File Descriptor**
프로세스에서 특정 파일(소켓, 파이프 등)에 접근할 때 사용하는 추상적인 값

**동작 방식**
- 프로세스가 실행 중에 파일을 열면 커널은 해당 프로세스의 file descriptor 숫자 중에 사용하지 않는 가장 작은 값을 할당해준다.
- 그 다음 프로세스가 열려있는 파일에 시스템 콜을 이용해 접근할 때, file descriptor 값을 이용해 파일을 지칭할 수 있다.

**기본 할당 FD**
- 0 : 표준 입력
- 1 : 표준 출력
- 2 : 표준 에러

**File System**
파일과 디렉터리를 보조기억장치에 일목요연하게 저장하고 접근할 수 있게 하는 운영체제 내부 프로그램
- 커널 영역에서 동작
- 계층 디렉터리 구조

메타 영역과 데이터 영역으로 나뉜다.
- 메타 영역 : 파일 데이터의 데이터가 저장된 영역(파일 이름, 위치, 크기, 유형 등)
- 데이터 영역 : 실제 데이터가 기록된 영역

## I-Node가 무엇인가요?
리눅스에서 파일 시스템을 관리하기 위해 사용되는 개체의 일종으로 파일의 정보를 가진다.

![image 9](https://github.com/user-attachments/assets/b5b2c4b4-f8e0-42f7-a1fb-0854300eff97)

**I-Node에 저장되는 정보**
1. 파일의 권환 : 파일의 소유자와 그룹에 대한 읽기, 쓰기, 실행 권한을 지정한다.
2. 소유자 정보 : 파일을 소유하는 사용자 ID, 그룸 ID를 저장한다.
3. 파일 크기 : 파일의 크기를 나타낸다.
4. 파일의 생성, 수정, 접근 시간(Time Stamps): 파일의 생성, 마지막 수정, 마지막 접근 시간을 저장하여 파일의 변경 이력을 관리한다.
5. 링크 수(Link Count): 해당 I-Node를 참조하는 하드 링크의 수를 저장합니다. 이 값이 0이 되면 파일 시스템은 I-Node와 파일 데이터를 삭제한다.
6. 데이터 위치에 대한 포인터(Pointers to Data Blocks): 실제 파일 데이터를 저장하는 데이터 블록의 위치 정보를 포함한다. I-Node는 데이터가 저장된 블록을 가리키는 포인터를 포함하여 파일이 저장된 위치를 참조할 수 있도록 한다.
## 프로그래밍 언어 상에서 제공하는 파일 관련 함수 (Python - open(), Java - BufferedReader/Writer 등)은, 파일을 어떤 방식으로 읽어들이나요?
- 시스템 콜 호출
- 운영 체제는 해당 파일의 디스크 위치를 확인하고 파일 디스크립터를 할당한다.
- 버퍼 크기만큼의 데이터를 한 번에 메모리로 읽어들인다.
- 버퍼에 있는 데이터를 프로그래밍 언어의 적절한 타입으로 변환한다.
# 23. 동기와 비동기, 블로킹과 논블로킹의 차이에 대해 설명해 주세요.


- **동기** : A는 B가 일을 하는 중에 기다리면서, 현재 상태가 어떤지 계속 체크한다.
- **비동기** : B의 수행 상태를 B 혼자 직접 신경쓰면서 처리한다.

![img1 daumcdn](https://github.com/user-attachments/assets/ce0f8fc8-03a9-4b45-9eab-26c7c6988853)

* **Blocking** : B는 내 할 일을 다 마칠 때까지 제어권을 가지고 있는다. A는 B가 다 마칠 때까지 기다려야 한다.
* **Non-blocking** : B는 할 일을 마치지 않았어도 A에게 제어권을 바로 넘겨준다. A는 B를 기다리면서도 다른 일을 진행할 수 있다.

> Non-blocking은 호출자 입장에서의 결과입니다. 입/출력을 다른 주체(예를 들어 OS 커널)에게 넘기고 호출자 자신은 다음 처리로 넘어가기 때문에 Blocking되지 않는 것이죠. 비동기는 Non-blocking 때문에 따라오는 결과적 현상 입니다. 입/출력이 언제 끝날지 호출자는 알 수 없기 때문이죠. 관점 차이로 보시면 이해해 도움이 되리라 생각합니다. 감사합니다


## 그렇다면, 동기이면서 논블로킹이고, 비동기이면서 블로킹인 경우는 의미가 있다고 할 수 있나요?
![img1 daumcdn 3](https://github.com/user-attachments/assets/35efa258-48a9-43dc-9552-4f7ba1f65fc0)

**Sync-NonBlocking**

<img width="640" alt="img1 daumcdn 5" src="https://github.com/user-attachments/assets/0dd5fc9a-f9d8-4e36-b82a-0d50a7f1fdea" />

논블로킹이므로 Thread1은 I/O가 끝나는 것을 기다리지 않는다. 하지만 동기 방식이므로 I/O를 시작한 스레드가 callback을 실행해야하므로 Task2가 길어진다면 callback은 큐에서 대기해야하므로 처리가 느려진다. Thread2는 작업이 가능함에도 불구하고 놀고 있게  된다.

<img width="640" alt="img1 daumcdn 6" src="https://github.com/user-attachments/assets/fee8c954-706b-4009-8798-ba077530cd27" />

Task2가 I/O보다 빨리 끝나면 바로 callback을 실행하므로 sync-blocking방식과 동일하게 동작하면서 기다리는 동안에 Task2도 처리할 수 있어 효율이 극대화될 수 있지만 Task2가 언제 끝날지 보장할 수 없으므로 잘 쓰이지 않는다.

**Async-Blocking**

<img width="640" alt="img1 daumcdn 4" src="https://github.com/user-attachments/assets/a41ac8a3-95f7-43f7-970f-5d8b3c745bde" />

호출 스레드가 I/O 완료를 기다리는데, 콜백은 다른 곳에서 실행할 수 있는 방식이다. I/O가 끝나자마자 반응할 수 있지만 I/O가 진행되는동안 스레드가 두개나 놀고 있는다.


## I/O 멀티플렉싱에 대해 설명해 주세요.
하나의 스레드가 여러 파일을 동시에 관리하는 것

- I/O 작업을 요청한 후에도 작업이 완료될 때까지 반복해서 상태를 확인한다.
- 여러 I/O 요청을 동시에 관리할 수 있는 방식으로 특정 I/O 작업이 완료되지 않더라도 해당 작업에서 대기하지 않고 즉시 다른 I/O 작업을 계속 수행한다.
-> 동기 - 논블로킹

## 논블로킹 I/O를 수행한다고 하면, 그 결과를 어떻게 수신할 수 있나요?

**폴링**
- 주기적으로 I/O 작업의 완료 여부를 확인한다.
- CPU 자원을 낭비할 수 있다.

**콜백 함수**
- I/O 작업이 완료될 때 실행될 콜백함수를 등록한다.
- 시스템이 자동으로 해당 콜백함수를 호출하여 결과를 처리할 수 있도록 한다.

**이벤트 nofitication**
- I/O 작업의 완료와 같은 이벤트가 발생했을 때 알림을 받는다.
